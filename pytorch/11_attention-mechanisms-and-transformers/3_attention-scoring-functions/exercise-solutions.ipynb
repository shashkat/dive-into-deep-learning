{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8d7d66",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0095fbcc",
   "metadata": {},
   "source": [
    "Disclaimer: These are just my (Shashank Katiyar's) answers while going through the d2l book. There is no guarantee of any kind of their correctness. Inputs/suggestions are most welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12edddc",
   "metadata": {},
   "source": [
    "##### 1. Implement distance-based attention by modifying the `DotProductAttention` code. Note that you only need the squared norms of the keys $||k_i||^2$ for an efficient implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af5518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "# declare some example variables\n",
    "batch_size = 100\n",
    "num_queries = 5\n",
    "num_keys_values = 10\n",
    "d = 3\n",
    "value_dimension = 2\n",
    "queries = torch.randn((batch_size, num_queries, d))\n",
    "keys = torch.randn((batch_size, num_keys_values, d))\n",
    "values = torch.randn((batch_size, num_keys_values, value_dimension))\n",
    "valid_lens = torch.randint(low = 1, high = num_keys_values + 1, size = (batch_size, num_queries)) # any entry of valid_lens cannot be greater than num_keys\n",
    "\n",
    "# masked softmax func\n",
    "def MaskedSoftMax(X, valid_lens):\n",
    "\t\"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "\t# X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "\t# shape of X: (num_batches, num_queries, num_keys)\n",
    "\t# if valid lens is 2d, it should have shape (num_batches, num_queries)\n",
    "\t# if valid lens is 1d, it should have the information for valid_lens for all queries corresponding to each batch, meaning shape (num_batches)\n",
    "\tdef _sequence_mask(X, valid_len, value=0):\n",
    "\t\tmaxlen = X.size(1) # this is num_keys, as X passed into this func is in 2d form (num_queries*num_batches, num_keys)\n",
    "\t\tmask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "\t\tX[~mask] = value\n",
    "\t\treturn X\n",
    "\tif valid_lens is None:\n",
    "\t\treturn nn.functional.softmax(X, dim=-1)\n",
    "\telse:\n",
    "\t\tshape = X.shape\n",
    "\t\tif valid_lens.dim() == 1:\n",
    "\t\t\tvalid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "\t\telse:\n",
    "\t\t\tvalid_lens = valid_lens.reshape(-1)\n",
    "\t# On the last axis, replace masked elements with a very large negative\n",
    "\t# value, whose exponentiation outputs 0\n",
    "\tX = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "\treturn nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "# define the distance-based version of dotproductattention\n",
    "class DotProductAttentionDistance(nn.Module):\n",
    "\t# define the __init__ method\n",
    "\tdef __init__(self, dropout):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\t# now define the forward method\n",
    "\t# Shape of queries: (batch_size, no. of queries, d)\n",
    "\t# Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "\t# Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "\t# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "\tdef forward(self, queries, keys, values, valid_lens = None):\n",
    "\t\t# first, we compute qkt - 1/2(||k||^2)\n",
    "\t\traw_weights = torch.bmm(queries, torch.transpose(keys, 1, 2)) - torch.square(torch.linalg.vector_norm(keys, dim = -1).unsqueeze(dim = 1))/2 # shape = (batch, queries, keys)\n",
    "\t\t# now, we correct the mean to be 0 (currently -d/2) and variance to be 1 (currently 3d/2)\n",
    "\t\td = queries.shape[2]\n",
    "\t\tscaled_weights = (raw_weights + d/2)/math.sqrt((3*d)/2)\n",
    "\t\t# now that we have the scaled weights, we compute masked softmax on it\n",
    "\t\tsoftmaxed_weights = MaskedSoftMax(scaled_weights, valid_lens)\n",
    "\t\t# finally, we have the softmax computed attention weights, and we can simply pass them through self.dropout, and multiply by values\n",
    "\t\treturn torch.bmm(self.dropout(softmaxed_weights), values) # shape = (batch, queries, value_dimension)\n",
    "\n",
    "temp = DotProductAttentionDistance(0.3)\n",
    "temp(queries, keys, values, valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85098b2c",
   "metadata": {},
   "source": [
    "Comments: We need to change the raw_weights (the attention-weight values in the first step, before scaling) from $q^{T}k_{i}$ to $q^{T}k_{i}-\\frac{1}{2}||k_{i}||^{2}$. Also, we need to recompute the mean and variance of this new term for the assumption $k_i$ and $q$ being randomly sampled from a normal distribution with mean 0 and variance 1, and correct appropriately. This answers it: https://www.perplexity.ai/search/k-and-q-are-two-vectors-both-h-yO44HwPrQIOr.j6AJXUoeg. Everything after that will be basically the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ef4b83",
   "metadata": {},
   "source": [
    "##### 2. Modify the dot product attention to allow for queries and keys of different dimensionalities by employing a matrix to adjust dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c303008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import dropout, nn\n",
    "\n",
    "# declare some example variables\n",
    "batch_size = 100\n",
    "num_queries = 5\n",
    "num_keys_values = 10\n",
    "query_dimension = 3\n",
    "key_dimension = 4\n",
    "value_dimension = 2\n",
    "queries = torch.randn((batch_size, num_queries, query_dimension))\n",
    "keys = torch.randn((batch_size, num_keys_values, key_dimension))\n",
    "values = torch.randn((batch_size, num_keys_values, value_dimension))\n",
    "valid_lens = torch.randint(low = 1, high = num_keys_values + 1, size = (batch_size, num_queries)) # any entry of valid_lens cannot be greater than num_keys\n",
    "\n",
    "# masked softmax func\n",
    "def MaskedSoftMax(X, valid_lens):\n",
    "\t\"\"\"Perform softmax operation by masking elements on the last axis.\"\"\"\n",
    "\t# X: 3D tensor, valid_lens: 1D or 2D tensor\n",
    "\t# shape of X: (num_batches, num_queries, num_keys)\n",
    "\t# if valid lens is 2d, it should have shape (num_batches, num_queries)\n",
    "\t# if valid lens is 1d, it should have the information for valid_lens for all queries corresponding to each batch, meaning shape (num_batches)\n",
    "\tdef _sequence_mask(X, valid_len, value=0):\n",
    "\t\tmaxlen = X.size(1) # this is num_keys, as X passed into this func is in 2d form (num_queries*num_batches, num_keys)\n",
    "\t\tmask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "\t\tX[~mask] = value\n",
    "\t\treturn X\n",
    "\tif valid_lens is None:\n",
    "\t\treturn nn.functional.softmax(X, dim=-1)\n",
    "\telse:\n",
    "\t\tshape = X.shape\n",
    "\t\tif valid_lens.dim() == 1:\n",
    "\t\t\tvalid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "\t\telse:\n",
    "\t\t\tvalid_lens = valid_lens.reshape(-1)\n",
    "\t# On the last axis, replace masked elements with a very large negative\n",
    "\t# value, whose exponentiation outputs 0\n",
    "\tX = _sequence_mask(X.reshape(-1, shape[-1]), valid_lens, value=-1e6)\n",
    "\treturn nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "class AdditiveAttention(nn.Module):\n",
    "\t# define the __init__ method\n",
    "\tdef __init__(self, dropout, query_dimension, key_dimension): # not implementing in the lazy way. Maybe later on can explore that possibility. For now, non-lazy seems fine.\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dropout = nn.Dropout(p = dropout)\n",
    "\t\t# declare the linear layer which will be used to deal with queries and keys of different dimensions\n",
    "\t\tself.W = nn.Linear(in_features = query_dimension, out_features = key_dimension, bias = False)\n",
    "\n",
    "\t# define the forward method\n",
    "\t# Shape of queries: (batch_size, no. of queries, d)\n",
    "\t# Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "\t# Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "\t# Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "\tdef forward(self, queries, keys, values, valid_lens):\n",
    "\t\t# just multiply the queries and keys with the W appropriately to get the raw attention weights\n",
    "\t\traw_weights = torch.bmm(self.W(queries), torch.transpose(keys, 1, 2))\n",
    "\t \t# not scaling to ensure 0 mean and 1 variance as it wasn't considered in the AdditiveAttention func in the book either\n",
    "\t  \t# just compute the masked softmax\n",
    "\t\tsoftmaxed_weights = MaskedSoftMax(raw_weights, valid_lens)\n",
    "\t  \t# now, pass the softmaxed_weights to the dropout layer, and multiply with values using torch bmm\n",
    "\t\treturn torch.bmm(self.dropout(softmaxed_weights), values)\n",
    "\n",
    "temp = AdditiveAttention(0.3, query_dimension, key_dimension)\n",
    "temp(queries, keys, values, valid_lens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a8ceb",
   "metadata": {},
   "source": [
    "Comments: In this, I adopted an easy approach by not considering any scaling on the raw_weights. This is partially because in the book also, they didn't do any scaling on the AdditiveAttention function's raw weights. Other than that, it was mostly simple to get the raw_weights using the linear layer in between q and k^T."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f10dce",
   "metadata": {},
   "source": [
    "##### 3. How does the computational cost scale with the dimensionality of the keys, queries, values, and their number? What about the memory bandwidth requirements?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aff744",
   "metadata": {},
   "source": [
    "Comments: Assuming the simple `DotProductAttention`, the main steps are: computation of $QK^t$, scaling it by dividing by $\\sqrt{d}$, performing softmax on it, and finally multiplying the softmaxed_weights by the value vectors. The time complexity of all of them is $nqueries \\cdot nkeys$ except computation of $QK^t$, which is $d \\cdot nqueries \\cdot nkeys$. Hence, total time complexity is $O(d \\cdot nqueries \\cdot nkeys)$.\n",
    "About memory usage, we are not storing any extra information at any time except the queries, keys and values tensors. Hence, the total memory usage is $O(nqueries \\cdot d + nkeys \\cdot d + nvalues \\cdot vdim + nqueries \\cdot nkeys)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
